services:
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-server
    command: >
      llama-server
      --hf-repo philip-weyer/Qwen3-VL-Embedding-8B-Q4_K_M-GGUF --hf-file qwen3-vl-embedding-8b-q4_k_m.gguf -c 2048
      --host 0.0.0.0
      --port 8080
      --embedding
    # -m /models/qwen3-vl-embedding-8b-q4_k_m.gguf
    # --mmproj /models/mmproj-qwen3-vl-embedding-8b.gguf
    volumes:
      - /data/models:/models:ro
      - /data/huggingface-cache:/cache
    ports:
      - "8080:8080"
    restart: unless-stopped