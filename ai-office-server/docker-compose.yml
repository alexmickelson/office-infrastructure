services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: openwebui
    ports:
      - "8080:8080"
    environment:
      - WEBUI_PORT=8080
      - DATABASE_URL=postgresql://openwebui:9hb02121-9br@openwebui-db:5432/openwebui_db
      - MCP_ENABLED=true
    volumes:
      - /data/openwebui:/app/backend/data
    restart: unless-stopped
    depends_on:
      openwebui-db:
        condition: service_started

  openwebui-db:
    image: postgres:17
    container_name: openwebui-db
    environment:
      POSTGRES_USER: openwebui
      POSTGRES_PASSWORD: 9hb02121-9br
      POSTGRES_DB: openwebui_db
    volumes:
      - /data/openwebui-pg:/var/lib/postgresql/data
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U openwebui"]
      interval: 5s
      timeout: 5s
      retries: 5

  # llama-swap:
  #   image: ghcr.io/mostlygeek/llama-swap:vulkan
  #   container_name: llama-swap
  #   restart: unless-stopped
  #   ports:
  #     - "9292:9292"
  #   volumes:
  #     - ./llamaswap/config.yml:/app/config.yaml
  #     - /data/llamaswap/models:/app/models
  #     - /home/alex/.lmstudio/models/lmstudio-community:/app/lmstudio-models:ro
  #   command: --listen :9292
  #   # privileged: true
  #   # cap_add:
  #   #   - SYS_ADMIN
  #   devices:
  #     - /dev/dri/renderD128:/dev/dri/renderD128
  #   # group_add:
  #   #   - 303 # render group in nixos?

  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-server
    command: >
      --port 8081
      --embedding
      --pooling mean
      -hf aiteza/Qwen3-VL-Embedding-8B-GGUF:Q8_0


    # -hf unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF:Q8_K_XL
    #   --n-gpu-layers 99 
    #   --jinja 
    #   --top-p 0.8 
    #   --top-k 20 
    #   --temp 0.7 
    #   --min-p 0.0 
    #   --flash-attn on 
    #   --presence-penalty 1.5 
    #   --ctx-size 8192

    # --hf-repo philip-weyer/Qwen3-VL-Embedding-8B-Q4_K_M-GGUF --hf-file qwen3-vl-embedding-8b-q4_k_m.gguf -c 2048
    volumes:
      - /data/models:/models:ro
      - /data/huggingface-cache:/root/.cache/llama.cpp
    ports:
      - "8081:8081"
    restart: unless-stopped
    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128
