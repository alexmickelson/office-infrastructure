services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: openwebui
    ports:
      - "8080:8080"
    environment:
      - WEBUI_PORT=8080
      - DATABASE_URL=postgresql://openwebui:9hb02121-9br@openwebui-db:5432/openwebui_db
      - MCP_ENABLED=true
    volumes:
      - /data/openwebui:/app/backend/data
    restart: unless-stopped

  openwebui-db:
    image: postgres:17
    container_name: openwebui-db
    environment:
      POSTGRES_USER: openwebui
      POSTGRES_PASSWORD: 9hb02121-9br
      POSTGRES_DB: openwebui_db
    volumes:
      - /data/openwebui-pg:/var/lib/postgresql/data
    restart: always

  # llama-swap:
  #   image: ghcr.io/mostlygeek/llama-swap:vulkan
  #   container_name: llama-swap
  #   restart: unless-stopped
  #   ports:
  #     - "9292:9292"
  #   volumes:
  #     - ./llamaswap/config.yml:/app/config.yaml
  #     - /data/llamaswap/models:/app/models
  #     - /home/alex/.lmstudio/models/lmstudio-community:/app/lmstudio-models:ro
  #   command: --listen :9292
  #   # privileged: true
  #   # cap_add:
  #   #   - SYS_ADMIN
  #   devices:
  #     - /dev/dri/renderD128:/dev/dri/renderD128
  #   # group_add:
  #   #   - 303 # render group in nixos?

  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-server
    command: >
      --hf-repo philip-weyer/Qwen3-VL-Embedding-8B-Q4_K_M-GGUF --hf-file qwen3-vl-embedding-8b-q4_k_m.gguf -c 2048
      --host 0.0.0.0
      --port 8080
      --embedding
    # -m /models/qwen3-vl-embedding-8b-q4_k_m.gguf
    # --mmproj /models/mmproj-qwen3-vl-embedding-8b.gguf
    environment:
      - HF_HOME=/hf-cache
    volumes:
      - /data/models:/models:ro
      - /data/huggingface-cache:/hf-cache
    ports:
      - "8080:8080"
    restart: unless-stopped
    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128