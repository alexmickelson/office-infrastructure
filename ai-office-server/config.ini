; https://huggingface.co/blog/ggml-org/model-management-in-llamacpp
; https://github.com/ggml-org/llama.cpp/pull/17859

; [Qwen3-Coder-480B-30B-A3B]
; model = /models/Qwen3-Coder-30B-A3B-Instruct-GGUF
; threads = -1
; ctx-size = 16384
; ; n-gpu-layers = 99
; ; ot = .ffn_.*_exps.=CPU
; temp = 0.7
; min-p = 0.0
; top-p = 0.8
; top-k = 20
; repeat-penalty = 1.05



[GLM-4.7-Flash]
model = /models/GLM-4.7-Flash-BF16-00001-of-00002.gguf
jinja = on
threads = -1
ctx-size = 8192
temp = 1.0
top-p = 0.95
min-p = 0.01
fit = on


[Qwen3-Next-80B-A3B]
model = /models/Qwen3-Next-80B-A3B-Thinking-Q4_K_S.gguf
; model = /models/Qwen3-Next-80B-A3B-Instruct-Q4_0.gguf
temp = 0.7
top-k = 20
min-p = 0.00 
top-p = 0.80
presence-penalty = 1.0
jinja = on
ctx-size = 32768
threads = -1