; https://huggingface.co/blog/ggml-org/model-management-in-llamacpp
; https://github.com/ggml-org/llama.cpp/pull/17859

; [Qwen3-Coder-480B-30B-A3B]
; model = /models/Qwen3-Coder-30B-A3B-Instruct-GGUF
; threads = -1
; ctx-size = 16384
; ; n-gpu-layers = 99
; ; ot = .ffn_.*_exps.=CPU
; temp = 0.7
; min-p = 0.0
; top-p = 0.8
; top-k = 20
; repeat-penalty = 1.05



[GLM-4.7-Flash]
model = /models/GLM-4.7-Flash-BF16-00001-of-00002.gguf
jinja = on
threads = -1
ctx-size = 8192
temp = 1.0
top-p = 0.95
min-p = 0.01
fit = on

[Qwen3-Next-80B-A3B]
model = /models/Qwen3-Next-80B-A3B-Thinking-Q4_K_S.gguf
; model = /models/Qwen3-Next-80B-A3B-Instruct-Q4_0.gguf
temp = 0.7
top-k = 20
min-p = 0.00 
top-p = 0.80
presence-penalty = 1.0
jinja = on
ctx-size = 32768
threads = -1

[Qwen3-Coder-30B-A3B]
model = /models/Qwen3-Coder-30B-A3B-Instruct-1M-BF16-00001-of-00002.gguf
temp = 0.7
top-k = 20
min-p = 0.00 
top-p = 0.80
repeat-penalty = 1.05
jinja = on
ctx-size = 32768


[Qwen3-VL-30B-A3B]
model = /models/Qwen3-VL-30B-A3B-Instruct-UD-Q8_K_XL.gguf
mmproj = /models/Qwen3-VL-30B-A3B-mmproj-BF16.gguf
jinja = on
top-k = 20
min-p = 0.00 
top-p = 0.8
temp = 0.7
flash-attn = on
presence-penalty = 1.5

; ctx-size = 256000

[Nemotron-3-Nano-30B-A3B]
model = /models/Nemotron-3-Nano-30B-A3B-BF16-00001-of-00002.gguf
jinja = on
top-p = 0.95
temp = 0.6
prio = 2
fit = on


; gpt oss 120b
; gpt oss 20b

; unsloth/Nemotron-3-Nano-30B-A3B-GGUF