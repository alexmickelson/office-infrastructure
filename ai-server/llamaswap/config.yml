models:
  "gpt-oss-120b":
    cmd: |
      ./llama-server \
      --port ${PORT} \
      -m models/gpt-oss-120b-F16.gguf \
      --ctx-size 0 --jinja -ub 2048 -b 2048
    # --ctx-size 32768 --jinja -ub 4096 -b 4096
    # --jinja --ctx-size 16384
    aliases:
      - "gpt-oss:120b"

  "gemma3-27b":
    cmd: |
      ./llama-server \
      --port ${PORT} \
      -m models/gemma-3-27b-it-UD-Q4_K_XL.gguf \
      --mmproj models/gemma-mmproj-F16.gguf \
      --ctx-size 0
    # -m models/gemma-3-27b-it-UD-Q4_K_XL.gguf \
    aliases:
      - "gemma3:27b"
    ttl: 300

# Groups configuration to run both models simultaneously
groups:
  "concurrent-models":
    # swap: false allows all models in this group to run at the same time
    swap: false
    # exclusive: false prevents this group from unloading other groups
    exclusive: false
    members:
      - "gpt-oss-120b"
      - "gemma3-27b"

# Hooks to preload both models on startup
hooks:
  on_startup:
    preload:
      - "gpt-oss-120b"
      - "gemma3-27b"
