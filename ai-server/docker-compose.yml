services:

  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    pull_policy: always
    container_name: openwebui
    network_mode: service:tailscale-ingress
    # ports:
    #   - "8080:8080"
    environment:
      - WEBUI_PORT=8080
      - OLLAMA_BASE_URL=http://localhost:11434
      - WEBUI_SECRET_KEY=
    volumes:
      - /data/openwebui:/app/backend/data
    restart: unless-stopped
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    pull_policy: always
    network_mode: service:tailscale-ingress
    # ports:
    #   - "11434:11434"
    volumes:
      - /data/ollama:/root/.ollama
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities:
                - gpu

  prometheus:
    image: bitnami/prometheus:3
    container_name: prometheus
    restart: unless-stopped
    # network_mode: service:tailscale-ingress # port 9090
    ports:
      - 9090:9090
    volumes:
      - ./prometheus.yml:/opt/bitnami/prometheus/conf/prometheus.yml
      - /data/prometheus:/opt/bitnami/prometheus/data
    extra_hosts:
      - host.docker.internal:host-gateway

  grafana:
    image: grafana/grafana:main
    container_name: grafana
    restart: unless-stopped
    env_file:
      - .env
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_SECURITY_ADMIN_USER=admin
      # - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - /data/grafana:/var/lib/grafana
      - ./grafana-datasource.yml:/etc/grafana/provisioning/datasources/grafana-datasource.yml:ro
    ports:
      - 3000:3000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/robots.txt"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 3s

  node_exporter:
    image: quay.io/prometheus/node-exporter:latest
    pull_policy: always
    container_name: node_exporter
    command:
      - '--path.rootfs=/host'
    network_mode: host
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'

  dcgm-exporter:
    image: nvidia/dcgm-exporter:4.2.3-4.1.3-ubuntu22.04
    pull_policy: always
    container_name: dcgm-exporter
    network_mode: host # port 9400
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    cap_add:
      - SYS_ADMIN
    environment:
      - NVIDIA_VISIBLE_DEVICES=all

  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    pull_policy: always
    container_name: pipelines
    restart: unless-stopped
    ports:
      - 9099:9099
    volumes:
      - /data/pipelines-openwebui:/app/pipelines
    extra_hosts:
      - host.docker.internal:host-gateway

  tailscale-ingress:
    image: tailscale/tailscale:latest
    pull_policy: always
    hostname: ai-snow
    env_file:
      - .env
    environment:
      # - TS_AUTHKEY= # from .env file
      - TS_SERVE_CONFIG=/config/ts-config.json
      - TS_STATE_DIR=/var/lib/tailscale
    volumes:
      - /data/tailscale-ingress:/var/lib/tailscale
      - ./ts-config.json:/config/ts-config.json
    extra_hosts:
      - host.docker.internal:host-gateway
    devices:
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - net_admin
    restart: always